{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Code 1 - Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 모듈을 불러온다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module Ready!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "print(\"Module Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Fold_Value는 CV Fold갯수를 의미함 \n",
    "#### 2. RANDOM_STATE는 실습을 위해 모델의 결과를 같게 하기 위함임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#############################\n",
    "FOLD_VALUE = 5\n",
    "RANDOM_STATE = 1026\n",
    "#############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사용할 Personal Loan 데이터셋을 불러옴\n",
    "> 1. 난수를 고정하여 8:2 = Training data : Test data로 나눔\n",
    "> 2. Training dataset의 column별 std와 mean을 이용하여 Train/Test dataset standardization 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Personal Loan' data column name :  ['ID', 'Age', 'Experience', 'Income', 'ZIP Code', 'Family', 'CCAvg', 'Education', 'Mortgage', 'Personal Loan', 'Securities Account', 'CD Account', 'Online', 'CreditCard']\n",
      "ID와 ZIP Code는 사용하지 않습니다. 또한 Personal Loan을 분류하는 binary classification 문제 입니다.\n",
      "Data partition complete! \n",
      "Train_Input shape : (2000, 11) \n",
      "Train_Output shape : (2000, 1)\n",
      "Test_Input shape : (500, 11) \n",
      "Test_Output shape : (500, 1)\n",
      "standardization complete!\n"
     ]
    }
   ],
   "source": [
    "# 사용할 Personal Loan 데이터셋을 불러옵니다.\n",
    "Rawdata = pd.read_csv('dataset/Personal Loan.csv')\n",
    "# Print Column names\n",
    "print(\"'Personal Loan' data column name : \", list(Rawdata.columns.values))\n",
    "print(\"ID와 ZIP Code는 사용하지 않습니다. 또한 Personal Loan을 분류하는 binary classification 문제 입니다.\")\n",
    "# Allocate column index based on Input and Output varaibles\n",
    "Input_Column_Index = np.concatenate((range(1,4),range(5,9),range(10,14)))\n",
    "Target_Column_Index = np.array([9])\n",
    "\n",
    "\n",
    "# 같은 데이터셋을 사용하기 위해서 난수를 고정합니다.\n",
    "np.random.seed(150)\n",
    "Train_Index = np.random.choice(np.shape(Rawdata)[0],int(np.shape(Rawdata)[0]*0.8),replace=False)\n",
    "\n",
    "# Input variable과 Output variable을 Numpy array로 변환합니다.\n",
    "Rawdata_Input = np.array(Rawdata)[:,Input_Column_Index]\n",
    "Rawdata_Output = np.array(Rawdata)[:,Target_Column_Index]\n",
    "\n",
    "\n",
    "# Training data와 Test data를 나누어 줍니다.\n",
    "Train_Input = Rawdata_Input[Train_Index,:]\n",
    "Train_Output = Rawdata_Output[Train_Index,:]\n",
    "Test_Input = Rawdata_Input[np.delete(range(np.shape(Rawdata)[0]),Train_Index),:]\n",
    "Test_Output = Rawdata_Output[np.delete(range(np.shape(Rawdata)[0]),Train_Index),:]\n",
    "print('Data partition complete! \\nTrain_Input shape :',np.shape(Train_Input),'\\nTrain_Output shape :',np.shape(Train_Output))\n",
    "print('Test_Input shape :',np.shape(Test_Input),'\\nTest_Output shape :',np.shape(Test_Output))\n",
    "\n",
    "# Input variable standardization based on Training data\n",
    "\n",
    "def standardization(Data,Data2):\n",
    "    return ((Data - np.mean(Data2, axis=0)) / np.std(Data2, axis=0))\n",
    "\n",
    "Train_Input_Normalized = copy.deepcopy(standardization(Train_Input,Train_Input))\n",
    "Test_Input_Normalized = copy.deepcopy(standardization(Test_Input,Train_Input))\n",
    "print(\"standardization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best hyperparameter를 찾기위하여 5-Fold 로 데이터를 나누며 CV할 함수를 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best Hyperparameter를 찾기위하여 5-Fold Cross Validation을 한다\n",
    "def k_Fold_Maker(InputData,OutputData,Partition_Number):\n",
    "    Index = 0\n",
    "    Input_List = list()\n",
    "    Output_List = list()\n",
    "    Length = int(np.floor(np.shape(InputData)[0]/Partition_Number))\n",
    "    for i in range(Partition_Number):\n",
    "        if(i == (Partition_Number-1)):\n",
    "            Input_List.append(InputData[range(Index+(Length* i), np.shape(InputData)[0]), :])\n",
    "            Output_List.append(OutputData[range(Index+(Length* i), np.shape(InputData)[0]), :])\n",
    "        else:\n",
    "            Input_List.append(InputData[range(Index + (Length * i), Index + (Length * (i + 1))), :])\n",
    "            Output_List.append(OutputData[range(Index + (Length * i), Index + (Length * (i + 1))), :])\n",
    "    return(Input_List,Output_List)\n",
    "\n",
    "# Make 5-Fold dataset for Cross validation\n",
    "Fold_Input, Fold_Output = k_Fold_Maker(Train_Input_Normalized, Train_Output, FOLD_VALUE)\n",
    "\n",
    "# Cross validation의 결과를 위한 함수\n",
    "def CV_Result_Each_Model(Hyper_Para,Model):    \n",
    "    FULL_Results = list() # 모든 결과를 담을 객체\n",
    "    for i in range(FOLD_VALUE):\n",
    "        Tr_Index = np.delete(range(FOLD_VALUE),i) #Training 에 사용할 Fold Index\n",
    "        Val_Index = i                    #Validation에 사용할 Fold Index\n",
    "\n",
    "        TRAIN_INPUT  = list()\n",
    "        TRAIN_OUTPUT = list()\n",
    "        for j in Tr_Index:\n",
    "            TRAIN_INPUT.append(Fold_Input[j])\n",
    "            TRAIN_OUTPUT.append(Fold_Output[j])\n",
    "        TRAIN_INPUT = np.concatenate(TRAIN_INPUT)\n",
    "        TRAIN_OUTPUT = np.concatenate(TRAIN_OUTPUT)\n",
    "        VALID_INPUT = Fold_Input[i]\n",
    "        VALID_OUTPUT = Fold_Output[i]\n",
    "        FULL_Results.append(Model(Hyper_Para,TRAIN_INPUT,TRAIN_OUTPUT,VALID_INPUT,VALID_OUTPUT))\n",
    "    print(\"CV Complete!\")\n",
    "    FULL_Results=np.concatenate(FULL_Results,axis=0)\n",
    "    return(FULL_Results)\n",
    "\n",
    "# 평가지표를 위한 함수를 생성한다.\n",
    "def Valid_Index(Data,NAME):\n",
    "    Accuracy = (Data[0,0]+Data[1,1])/np.sum(Data)\n",
    "    TPR = Data[1,1]/np.sum(Data[1,:])\n",
    "    TNR = Data[0,0]/np.sum(Data[0,:])\n",
    "    Precision = Data[1,1]/np.sum(Data[:,1])\n",
    "    BCR = np.sqrt(TPR*TNR)\n",
    "    F1 = (2*TPR*Precision)/(TPR+Precision)\n",
    "    TMP=pd.DataFrame({'Model' : NAME,\n",
    "                  'Accuracy' : [Accuracy],\n",
    "                  'TPR': [TPR],\n",
    "                  'TNR': [TNR],\n",
    "                  'Precision': [Precision],\n",
    "                  'BCR': [BCR],\n",
    "                  'F1': [F1]})\n",
    "    Results=TMP[['Model','Accuracy','F1','BCR','Precision','TPR','TNR']]\n",
    "    return(Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![NN_Description](dataset/NeuralNetwork_Description.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model1. Neural network\n",
    "## Neuralnetwork 함수와 5-Fold CV를 위해 함수를 생성 후 이행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>BCR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One_Layer_Node_20</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>0.860892</td>\n",
       "      <td>0.901252</td>\n",
       "      <td>0.906077</td>\n",
       "      <td>0.820</td>\n",
       "      <td>0.990556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two_Layer_Node_20</td>\n",
       "      <td>0.9705</td>\n",
       "      <td>0.849873</td>\n",
       "      <td>0.907160</td>\n",
       "      <td>0.865285</td>\n",
       "      <td>0.835</td>\n",
       "      <td>0.985556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One_Layer_Node_10</td>\n",
       "      <td>0.9715</td>\n",
       "      <td>0.848000</td>\n",
       "      <td>0.887656</td>\n",
       "      <td>0.908571</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.991111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three_Layer_Node_10</td>\n",
       "      <td>0.9695</td>\n",
       "      <td>0.844784</td>\n",
       "      <td>0.904185</td>\n",
       "      <td>0.860104</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.985000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Three_Layer_Node_20</td>\n",
       "      <td>0.9690</td>\n",
       "      <td>0.837696</td>\n",
       "      <td>0.888944</td>\n",
       "      <td>0.879121</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.987778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Two_Layer_Node_10</td>\n",
       "      <td>0.9680</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.888444</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.800</td>\n",
       "      <td>0.986667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy        F1       BCR  Precision    TPR  \\\n",
       "0    One_Layer_Node_20    0.9735  0.860892  0.901252   0.906077  0.820   \n",
       "0    Two_Layer_Node_20    0.9705  0.849873  0.907160   0.865285  0.835   \n",
       "0    One_Layer_Node_10    0.9715  0.848000  0.887656   0.908571  0.795   \n",
       "0  Three_Layer_Node_10    0.9695  0.844784  0.904185   0.860104  0.830   \n",
       "0  Three_Layer_Node_20    0.9690  0.837696  0.888944   0.879121  0.800   \n",
       "0    Two_Layer_Node_10    0.9680  0.833333  0.888444   0.869565  0.800   \n",
       "\n",
       "        TNR  \n",
       "0  0.990556  \n",
       "0  0.985556  \n",
       "0  0.991111  \n",
       "0  0.985000  \n",
       "0  0.987778  \n",
       "0  0.986667  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################\n",
    "# Neural Network Hyperparameter Set\n",
    "###############################################\n",
    "ACTIVATION = 'tanh'        \n",
    "SOLVER = 'adam'            \n",
    "BATCH_SIZE = 32            \n",
    "HIDDEN_LAYER= [10,10,10]   \n",
    "TR_INPUT = Train_Input     \n",
    "TR_OUTPUT = Train_Output[:,0]\n",
    "Iterlation = 3000\n",
    "L2_Penalty = 0.001\n",
    "Visualization = False \n",
    "Validation_Percent = 0.0 \n",
    "Decay_Method = 'invscaling'\n",
    "Power_Value = 0.5         \n",
    "Tolerence_Value = 1e-04 \n",
    "###############################################\n",
    "\n",
    "def NeuralNetwork(HIDDEN_LAYER,Train_Input,TR_OUTPUT,Val_Input,Val_Output):\n",
    "    MLP=MLPClassifier(activation=ACTIVATION,solver=SOLVER,alpha=L2_Penalty,\n",
    "                      hidden_layer_sizes=HIDDEN_LAYER,\n",
    "                      batch_size=BATCH_SIZE,max_iter=Iterlation,verbose=Visualization,early_stopping=False,power_t=Power_Value,\n",
    "                      validation_fraction=Validation_Percent,learning_rate=Decay_Method,tol=Tolerence_Value,\n",
    "                      random_state =RANDOM_STATE).fit(Train_Input,TR_OUTPUT[:,0])\n",
    "    Predict_Value = MLP.predict(Val_Input)\n",
    "    return(np.concatenate((Predict_Value[:,np.newaxis],Val_Output),axis=1))\n",
    "\n",
    "\n",
    "# 자 이제 NeuralNet을 기준으로 파라미터별 5-Fold CV를 해보도록 한다.\n",
    "One_Layer_Node_10 = CV_Result_Each_Model([10],NeuralNetwork)\n",
    "One_Layer_Node_20 = CV_Result_Each_Model([20],NeuralNetwork)\n",
    "Two_Layer_Node_10 = CV_Result_Each_Model([10,10],NeuralNetwork)\n",
    "Two_Layer_Node_20 = CV_Result_Each_Model([20,20],NeuralNetwork)\n",
    "Three_Layer_Node_10 = CV_Result_Each_Model([10,10,10],NeuralNetwork)\n",
    "Three_Layer_Node_20 = CV_Result_Each_Model([20,20,20],NeuralNetwork)\n",
    "\n",
    "ANN_5CV_Results=Valid_Index(confusion_matrix(One_Layer_Node_10[:,1],One_Layer_Node_10[:,0]),\"One_Layer_Node_10\").append([\n",
    "Valid_Index(confusion_matrix(One_Layer_Node_20[:,1],One_Layer_Node_20[:,0]),\"One_Layer_Node_20\"),\n",
    "Valid_Index(confusion_matrix(Two_Layer_Node_10[:,1],Two_Layer_Node_10[:,0]),\"Two_Layer_Node_10\"),\n",
    "Valid_Index(confusion_matrix(Two_Layer_Node_20[:,1],Two_Layer_Node_20[:,0]),\"Two_Layer_Node_20\"),\n",
    "Valid_Index(confusion_matrix(Three_Layer_Node_10[:,1],Three_Layer_Node_10[:,0]),\"Three_Layer_Node_10\"),\n",
    "Valid_Index(confusion_matrix(Three_Layer_Node_20[:,1],Three_Layer_Node_20[:,0]),\"Three_Layer_Node_20\")])\n",
    "\n",
    "ANN_5CV_Results = ANN_5CV_Results.sort_values(by=['F1'],ascending=False)\n",
    "pd.DataFrame(ANN_5CV_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model2. RandomForest\n",
    "## RandomForest 함수 5-Fold CV를 위해 함수 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>BCR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF150_CV</td>\n",
       "      <td>0.9840</td>\n",
       "      <td>0.914439</td>\n",
       "      <td>0.923891</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.855</td>\n",
       "      <td>0.998333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF200_CV</td>\n",
       "      <td>0.9835</td>\n",
       "      <td>0.911528</td>\n",
       "      <td>0.921186</td>\n",
       "      <td>0.982659</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.998333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF100_CV</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.920929</td>\n",
       "      <td>0.977011</td>\n",
       "      <td>0.850</td>\n",
       "      <td>0.997778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RF50_CV</td>\n",
       "      <td>0.9830</td>\n",
       "      <td>0.908602</td>\n",
       "      <td>0.918472</td>\n",
       "      <td>0.982558</td>\n",
       "      <td>0.845</td>\n",
       "      <td>0.998333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Accuracy        F1       BCR  Precision    TPR       TNR\n",
       "0  RF150_CV    0.9840  0.914439  0.923891   0.982759  0.855  0.998333\n",
       "0  RF200_CV    0.9835  0.911528  0.921186   0.982659  0.850  0.998333\n",
       "0  RF100_CV    0.9830  0.909091  0.920929   0.977011  0.850  0.997778\n",
       "0   RF50_CV    0.9830  0.908602  0.918472   0.982558  0.845  0.998333"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def RandomForest(NUMBER,Train_Input,TR_OUTPUT,Val_Input,Val_Output):\n",
    "    RF=RandomForestClassifier(n_estimators=NUMBER,\n",
    "                           max_features=\"sqrt\",\n",
    "                           random_state=RANDOM_STATE).fit(Train_Input,TR_OUTPUT[:,0])\n",
    "    Predict_Value = RF.predict(Val_Input)\n",
    "    return(np.concatenate((Predict_Value[:,np.newaxis],Val_Output),axis=1))\n",
    "\n",
    "RF50_CV = CV_Result_Each_Model(50,RandomForest)\n",
    "RF100_CV = CV_Result_Each_Model(100,RandomForest)\n",
    "RF150_CV = CV_Result_Each_Model(150,RandomForest)\n",
    "RF200_CV = CV_Result_Each_Model(200,RandomForest)\n",
    "\n",
    "ANN_5CV_Results=Valid_Index(confusion_matrix(RF50_CV[:,1],RF50_CV[:,0]),\"RF50_CV\").append([\n",
    "Valid_Index(confusion_matrix(RF100_CV[:,1],RF100_CV[:,0]),\"RF100_CV\"),\n",
    "Valid_Index(confusion_matrix(RF150_CV[:,1],RF150_CV[:,0]),\"RF150_CV\"),\n",
    "Valid_Index(confusion_matrix(RF200_CV[:,1],RF200_CV[:,0]),\"RF200_CV\")])\n",
    "\n",
    "ANN_5CV_Results = ANN_5CV_Results.sort_values(by=['F1'],ascending=False)\n",
    "pd.DataFrame(ANN_5CV_Results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model3. Adaboost\n",
    "## Adaboost함수와 5-Fold CV를 위해 함수를 생성 후 이행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>BCR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada_300</td>\n",
       "      <td>0.9575</td>\n",
       "      <td>0.776903</td>\n",
       "      <td>0.852311</td>\n",
       "      <td>0.817680</td>\n",
       "      <td>0.740</td>\n",
       "      <td>0.981667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada_50</td>\n",
       "      <td>0.9580</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.841903</td>\n",
       "      <td>0.837209</td>\n",
       "      <td>0.720</td>\n",
       "      <td>0.984444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada_1000</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.774026</td>\n",
       "      <td>0.854459</td>\n",
       "      <td>0.805405</td>\n",
       "      <td>0.745</td>\n",
       "      <td>0.980000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada_200</td>\n",
       "      <td>0.9565</td>\n",
       "      <td>0.770449</td>\n",
       "      <td>0.846532</td>\n",
       "      <td>0.815642</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.981667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ada_100</td>\n",
       "      <td>0.9570</td>\n",
       "      <td>0.767568</td>\n",
       "      <td>0.836036</td>\n",
       "      <td>0.835294</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.984444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Model  Accuracy        F1       BCR  Precision    TPR       TNR\n",
       "0   Ada_300    0.9575  0.776903  0.852311   0.817680  0.740  0.981667\n",
       "0    Ada_50    0.9580  0.774194  0.841903   0.837209  0.720  0.984444\n",
       "0  Ada_1000    0.9565  0.774026  0.854459   0.805405  0.745  0.980000\n",
       "0   Ada_200    0.9565  0.770449  0.846532   0.815642  0.730  0.981667\n",
       "0   Ada_100    0.9570  0.767568  0.836036   0.835294  0.710  0.984444"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def AdaBoost(NUMBER,Train_Input,TR_OUTPUT,Val_Input,Val_Output):\n",
    "    ADA=AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),n_estimators=NUMBER,algorithm=\"SAMME\",\n",
    "                           random_state=RANDOM_STATE).fit(Train_Input,TR_OUTPUT[:,0])\n",
    "    Predict_Value = ADA.predict(Val_Input)\n",
    "    return(np.concatenate((Predict_Value[:,np.newaxis],Val_Output),axis=1))\n",
    "\n",
    "Adaboost_50 = CV_Result_Each_Model(50,AdaBoost)\n",
    "Adaboost_100 = CV_Result_Each_Model(100,AdaBoost)\n",
    "Adaboost_200 = CV_Result_Each_Model(200,AdaBoost)\n",
    "Adaboost_300 = CV_Result_Each_Model(300,AdaBoost)\n",
    "Adaboost_1000 = CV_Result_Each_Model(1000,AdaBoost)\n",
    "\n",
    "ADA_5CV_Results=Valid_Index(confusion_matrix(Adaboost_50[:,1],Adaboost_50[:,0]),\"Ada_50\").append([\n",
    "Valid_Index(confusion_matrix(Adaboost_100[:,1],Adaboost_100[:,0]),\"Ada_100\"),\n",
    "Valid_Index(confusion_matrix(Adaboost_200[:,1],Adaboost_200[:,0]),\"Ada_200\"),\n",
    "Valid_Index(confusion_matrix(Adaboost_300[:,1],Adaboost_300[:,0]),\"Ada_300\"),\n",
    "Valid_Index(confusion_matrix(Adaboost_1000[:,1],Adaboost_1000[:,0]),\"Ada_1000\")])\n",
    "\n",
    "ADA_5CV_Results = ADA_5CV_Results.sort_values(by=['F1'],ascending=False)\n",
    "ADA_5CV_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model4. Bagging ANN\n",
    "## Bagging ANN함수와 5-Fold CV를 위해 함수를 생성 후 이행\n",
    "> 30번 반복 연산하며 오래 걸리므로 (2 Layer/20 Node), (1 Layer/20 Node) 2가지만 시행해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Complete!\n",
      "CV Complete!\n"
     ]
    }
   ],
   "source": [
    "def B_NeuralNetwork(HIDDEN_LAYER,Train_Input,TR_OUTPUT,Val_Input,Val_Output):\n",
    "    MLP=MLPClassifier(activation=ACTIVATION,solver=SOLVER,alpha=L2_Penalty,\n",
    "                      hidden_layer_sizes=HIDDEN_LAYER,max_iter=3000,\n",
    "                      batch_size=BATCH_SIZE,verbose=Visualization,early_stopping=False,power_t=Power_Value,\n",
    "                      validation_fraction=Validation_Percent,learning_rate=Decay_Method,tol=Tolerence_Value,random_state =RANDOM_STATE)\n",
    "    BMLP = BaggingClassifier(n_estimators=30, base_estimator=MLP, random_state=RANDOM_STATE, n_jobs=-1).fit(Train_Input, TR_OUTPUT[:, 0])\n",
    "    Predict_Value = BMLP.predict(Val_Input)\n",
    "    return(np.concatenate((Predict_Value[:,np.newaxis],Val_Output),axis=1))\n",
    "\n",
    "B_ANN_Two_20 = CV_Result_Each_Model([20,20],B_NeuralNetwork)\n",
    "B_ANN_One_20 = CV_Result_Each_Model([20],B_NeuralNetwork) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>BCR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_ANN_Two_20</td>\n",
       "      <td>0.9735</td>\n",
       "      <td>0.859416</td>\n",
       "      <td>0.896242</td>\n",
       "      <td>0.915254</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.991667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_ANN_One_20</td>\n",
       "      <td>0.9730</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>0.888402</td>\n",
       "      <td>0.924419</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.992778</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model  Accuracy        F1       BCR  Precision    TPR       TNR\n",
       "0  B_ANN_Two_20    0.9735  0.859416  0.896242   0.915254  0.810  0.991667\n",
       "0  B_ANN_One_20    0.9730  0.854839  0.888402   0.924419  0.795  0.992778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_ANN_Results=pd.concat((Valid_Index(confusion_matrix(B_ANN_Two_20[:,1],B_ANN_Two_20[:,0]),\"B_ANN_Two_20\"),\n",
    "          Valid_Index(confusion_matrix(B_ANN_One_20[:,1],B_ANN_One_20[:,0]),\"B_ANN_One_20\")))\n",
    "\n",
    "B_ANN_Results = B_ANN_Results.sort_values(by=['F1'],ascending=False)\n",
    "B_ANN_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model5. Bagging Decision Tree\n",
    "## Bagging Decision Tree함수와 5-Fold CV를 위해 함수를 생성 후 이행\n",
    ">  depth=5, depth=6 2가지만 시행해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Complete!\n",
      "CV Complete!\n"
     ]
    }
   ],
   "source": [
    "def B_Tree(NUMBER,Train_Input,TR_OUTPUT,Val_Input,Val_Output):\n",
    "    Tree = DecisionTreeClassifier(max_depth=NUMBER, random_state=RANDOM_STATE).fit(Train_Input, TR_OUTPUT[:, 0])\n",
    "    BMLP = BaggingClassifier(n_estimators=30, base_estimator=Tree, random_state=RANDOM_STATE, n_jobs=-1).fit(Train_Input, TR_OUTPUT[:, 0])\n",
    "    Predict_Value = BMLP.predict(Val_Input)\n",
    "    return(np.concatenate((Predict_Value[:,np.newaxis],Val_Output),axis=1))\n",
    "\n",
    "B_Tree_D6=CV_Result_Each_Model(6,B_Tree)\n",
    "B_Tree_D5=CV_Result_Each_Model(5,B_Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>BCR</th>\n",
       "      <th>Precision</th>\n",
       "      <th>TPR</th>\n",
       "      <th>TNR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_Tree_D6</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.930143</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.994444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B_Tree_D5</td>\n",
       "      <td>0.982</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.925299</td>\n",
       "      <td>0.955556</td>\n",
       "      <td>0.86</td>\n",
       "      <td>0.995556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Model  Accuracy        F1       BCR  Precision   TPR       TNR\n",
       "0  B_Tree_D6     0.982  0.906250  0.930143   0.945652  0.87  0.994444\n",
       "0  B_Tree_D5     0.982  0.905263  0.925299   0.955556  0.86  0.995556"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_Tree_Results=pd.concat((Valid_Index(confusion_matrix(B_Tree_D6[:,1],B_Tree_D6[:,0]),\"B_Tree_D6\"),\n",
    "          Valid_Index(confusion_matrix(B_Tree_D5[:,1],B_Tree_D5[:,0]),\"B_Tree_D5\")))\n",
    "\n",
    "B_Tree_Results = B_Tree_Results.sort_values(by=['F1'],ascending=False)\n",
    "B_Tree_Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model6. Gradient Boosting Machine\n",
    "## Bagging Decision Tree함수와 5-Fold CV를 위해 함수를 생성 후 이행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "CV Complete!\n",
      "     Model  Accuracy        F1       BCR  Precision    TPR       TNR\n",
      "0  GBM_100    0.9860  0.927461  0.944203   0.962366  0.895  0.996111\n",
      "0  GBM_300    0.9850  0.922280  0.941299   0.956989  0.890  0.995556\n",
      "0  GBM_150    0.9845  0.919897  0.941037   0.951872  0.890  0.995000\n",
      "0  GBM_250    0.9845  0.919897  0.941037   0.951872  0.890  0.995000\n",
      "0  GBM_200    0.9845  0.919481  0.938652   0.956757  0.885  0.995556\n"
     ]
    }
   ],
   "source": [
    "def GBM(NUMBER,Train_Input,TR_OUTPUT,Val_Input,Val_Output):\n",
    "    gbm=GradientBoostingClassifier(n_estimators=NUMBER,random_state=RANDOM_STATE).fit(Train_Input,TR_OUTPUT[:,0])\n",
    "    Predict_Value = gbm.predict(Val_Input)\n",
    "    return(np.concatenate((Predict_Value[:,np.newaxis],Val_Output),axis=1))\n",
    "\n",
    "GBM_100 = CV_Result_Each_Model(100,GBM)\n",
    "GBM_150 = CV_Result_Each_Model(150,GBM)\n",
    "GBM_200 = CV_Result_Each_Model(200,GBM)\n",
    "GBM_250 = CV_Result_Each_Model(250,GBM)\n",
    "GBM_300 = CV_Result_Each_Model(300,GBM)\n",
    "\n",
    "GBM_5CV_Results=Valid_Index(confusion_matrix(GBM_100[:,1],GBM_100[:,0]),\"GBM_100\").append([\n",
    "Valid_Index(confusion_matrix(GBM_150[:,1],GBM_150[:,0]),\"GBM_150\"),\n",
    "Valid_Index(confusion_matrix(GBM_200[:,1],GBM_200[:,0]),\"GBM_200\"),\n",
    "Valid_Index(confusion_matrix(GBM_250[:,1],GBM_250[:,0]),\"GBM_250\"),\n",
    "Valid_Index(confusion_matrix(GBM_300[:,1],GBM_300[:,0]),\"GBM_300\")])\n",
    "\n",
    "GBM_5CV_Results = GBM_5CV_Results.sort_values(by=['F1'],ascending=False)\n",
    "print(GBM_5CV_Results)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "hynix",
   "language": "python",
   "name": "lab7"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
